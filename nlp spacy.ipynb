{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Spacy tool\n",
    "#spaCy is an open-source software library for advanced natural language processing,\n",
    "#written in the programming languages Python and Cython\n",
    "#It is designed with the applied data scientist in mind, meaning it does not weigh the user down with decisions over what esoteric algorithms to use for common tasks and it‚Äôs fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19MAI0019 ARUNVARDHAN REDDY DA-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "a=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whole documents\n",
    "text = (\"Indian embassy officials here have said the speech of Prime Minister  Modi to chief ministers on June 18 as well as the comments by the spokesperson of the Ministry of Foreign Affairs have been deleted from two Chinese \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = a(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Indian embassy officials', 'the speech', 'Prime Minister  Modi', 'chief ministers', 'June', 'the comments', 'the spokesperson', 'the Ministry', 'Foreign Affairs', 'two Chinese']\n",
      "Verbs: ['say', 'delete']\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian NORP\n",
      "Modi PERSON\n",
      "June 18 DATE\n",
      "the Ministry of Foreign Affairs ORG\n",
      "two CARDINAL\n",
      "Chinese NORP\n"
     ]
    }
   ],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian\n",
      "Cricket\n",
      "Team\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Indian Cricket Team\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian Indian PROPN NNP compound Xxxxx True False\n",
      "Cricket Cricket PROPN NNP compound Xxxxx True False\n",
      "Team Team PROPN NNP ROOT Xxxx True False\n"
     ]
    }
   ],
   "source": [
    "#partsofspeech tags and dependies\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian 0 6 NORP\n",
      "Modi 70 74 PERSON\n",
      "June 18 97 104 DATE\n",
      "the Ministry of Foreign Affairs 152 183 ORG\n",
      "two 207 210 CARDINAL\n",
      "Chinese 211 218 NORP\n"
     ]
    }
   ],
   "source": [
    "#Named Entities\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Indian embassy officials here have said the speech of Prime Minister  Modi to chief ministers on June 18 as well as the comments by the spokesperson of the Ministry of Foreign Affairs have been deleted from two Chinese\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach\n",
      "emoji\n",
      "üçë\n",
      "outranking eggplant\n",
      "Peach\n",
      "Peach is the superior emoji.\n"
     ]
    }
   ],
   "source": [
    "#GET Tokens,noun chunks and sentences\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          \"emoji. It's outranking eggplant üçë \")\n",
    "print(doc[0].text)          # 'Peach'\n",
    "print(doc[1].text)          # 'emoji'\n",
    "print(doc[-1].text)         # 'üçë'\n",
    "print(doc[17:19].text)      # 'outranking eggplant'\n",
    "\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # 'Peach emoji'\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print(sentences[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13756031414435721121 India\n",
      "13756031414435721121 13756031414435721121\n",
      "India India\n",
      "2318904499095803474 deer\n"
     ]
    }
   ],
   "source": [
    "#using hash values for string\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love India\")\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[\"India\"]  # 3197928453018144401\n",
    "coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'\n",
    "print(coffee_hash, coffee_text)\n",
    "print(doc[2].orth, coffee_hash)  # 3197928453018144401\n",
    "print(doc[2].text, coffee_text)  # 'coffee'\n",
    "\n",
    "deer_hash = doc.vocab.strings.add(\"deer\")  # 3073001599257881079\n",
    "deer_text = doc.vocab.strings[deer_hash]  # 'beer'\n",
    "print(deer_hash, deer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Update modeling using spacy\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "train_data = [(\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link:https://spacy.io/usage/spacy-101#whats-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sen=\"nifnnfjin\"\n",
    "grammer=\"kf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'Regexparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-99d5f2718d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'Regexparser'"
     ]
    }
   ],
   "source": [
    "cp=nltk.Regexparser(grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
